# Overview

The key goal of research data science is to learn from data. One of the most powerful methods of learning from data is **modelling**.

In modelling you build mathematical descriptions of the processes that generate your data. By doing so, researchers can see __beyond__ the data and peek into the phenomeon that gave rise to your data. 

Modelling is powerful. Data becomes greater than the sum of its parts, instead becoming a tool with which we can make prediction machines for future unobserved data. This power means that modelling has applications in pretty much any data science problem.

However, modelling can also seem like a magic black box. How does maths learn from data? What does it mean to fit a model? 

This module provides a high-level introduction to modelling. We aim to demystify the key concepts involved, providing a foundational approach to modelling that one could apply to any modelling problem. In doing so we will also cover the main pitfalls that any modeller needs to contend with. Of course, every generaliseable abstract concept is only really understood through application. Here we use simple models (linear and logistic regression) to bring these concepts to life, but the intended take-homes are not specific to any particular modelling technique. 


The module is structured as follows: 

- **The what and why of modelling.** We begin by defining what modelling is and motivating the power of modelling.
- **The how: building a model**. Here we go through the components of a model, including describing how to fit one to data.
- **Interpreting a model**. We next cover how to understand what your model has learned about your data.
- **Validating a model**. It is not enough to have a model that is fitted to your data. The model has to be useful.
- **Improving a model**. Modelling is a process of iterative improvement.


**References:**

We will include more specific references as we move through the module. But useful accessible introductions to modelling that has inspired much of this module's content are Poldrack's [Statistical Thinking for the 21st Century](https://web.stanford.edu/group/poldracklab/statsthinking21/index.html), Holmes and Huber's [Modern Statistics for Modern Biology](https://web.stanford.edu/class/bios221/book/Chap-Models.html), as well as the introductory sections of Richard McElreath's wonderful [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) and Bishop's classic [Machine Learning for Pattern Recognition](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) textbook.


