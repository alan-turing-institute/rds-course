# The What and Why of Statistical Modelling

Models are used in all walks of life to facilitate learning. A product designer may build a mock-up of the intended product to learn how the real-life counterpart may work. Similarly, architects use blueprints to learn and communicate something about a building.

These models are abstractions of the real-world that capture key characteristics of the thing they imitate. 

Statistical models are no different. They attempt to capture key characteristics of **data generating processes** so that we can learn about the associated **phenomenon**. To understand models first we need to think about data for a moment.

### What is data? 

Data are a messy peek into an underlying phenomenon. What we really want to learn about is a **phenomenon**, for example what contributes to good health. However, the real-world is complicated and phenomena are never known precisely. Many natural phenomena have intrinsic variability. For example, human bodies are massively variable between individual, so it is reasonable to expect that their health, and also the myriad factors that contribute to it, will be different. 

Further, phenomena are only ever accessed through a process of **measurement**. Measurement brings it's own uncertainty since the measure itself could be inaccurate or imprecise (we discussed the ethical implications of this in Module 1). 

![](https://i.imgur.com/PQ6zYeY.png)
![](https://i.imgur.com/GQJCB33.png)


 Measurement, or observation, provides a set of data. Data are _related_ to the underlying phenomenon, but have been contorted by sampling bias (intrinsic variability will mean that each sample will be different) and measurement uncertainty. 
 
 Together, the phenomenon and method of observation make up the **true generating process**. 


 ### What is probability? 

 