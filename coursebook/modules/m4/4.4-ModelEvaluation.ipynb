{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true},"source":["(section4.4)=\n","# 4.4 Evaluating models\n","\n","\n","In the last section we created a very simple model linking the binarised self reported health with the deprivation index, controlled by age, for the UK. We emphasised understanding the model and developed intuition of what the model learns, but a modelling task is not complete without evaluation. Evaluation examines how well the model has learned the data and is able generalise it to unseen data.\n","\n","One of the evaluation tools available to us are **goodness-of-fit** metrics. These metrics summarise the discrepancy between observed values from the data used for fitting and the values expected under the estimated parameters of the model. \n","\n","Goodness of fit metrics depend of the kind of model you are fitting, e.g., in regression you tend to look at the *Mean Square Error*, *Root Mean Squared Error*, *Coefficient of Determination*, residual plots, etc. All these metrics have to be interpreted with the full knowledge of the context of the data and the model. You can find more about these metrics in [here](https://medium.com/microsoftazure/how-to-better-evaluate-the-goodness-of-fit-of-regressions-990dbf1c0091)). \n","\n","The **goodness-of-fit** metrics alone are not enough for a full evaluation of the model. A fit can learn the data perfectly but can be **overfitted**, meaning that they have learned the peculiarities (noise) of the dataset and will make poor predictions on future unseen samples (we touched on overfitting in [_Section 4.2_](section4.2)). Hence, part of model evaluation is to estimate the generalization accuracy of a model on unseen/out-of-sample data by using a **test set** (i.e data not seen by the model).\n","\n","In this section we will build on the modelling steps done in [_Section 4.3_](section4.3) and perform model evaluation.  In this example we are focusing on logistic regression, however we will try to make the concepts and ideas generalisable to other models. "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import statsmodels.api as sm\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve, precision_recall_curve\n","from sklearn import metrics\n","from matplotlib import pyplot\n","from scipy import stats\n","\n","plt.style.use('seaborn')\n","sns.set_theme(style=\"whitegrid\")\n","sns.set_style(\"white\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data processing\n","\n","**Note**: *We aim that each section can function as a stand-alone material, therefore we repeat the data processing steps in Sections 3.5, 4.3 and now here. Feel free to skip to the next section.*\n","\n","We can access the data by downloading the [csv](https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=7724#!/details) option from here.\n","\n","Unzip the data to `$PROJECT_ROOT/data` (where `$PROJECT_ROOT` is the root of the cloned github repository for this course).\n","This should give you `$PROJECT_ROOT/data/UKDA-7724-csv` directory.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../../../data/UKDA-7724-csv/csv/eqls_2011.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/f4/m42_v8vj1fjc0wrwtb69xjt80000gr/T/ipykernel_12203/1131236024.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatafolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../data/UKDA-7724-csv/'\u001b[0m \u001b[0;31m# should match the path you unzipped the data to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'csv/eqls_2011.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mrdoc/excel/eqls_api_map.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/Caskroom/miniforge/base/envs/playground/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/UKDA-7724-csv/csv/eqls_2011.csv'"]}],"source":["datafolder = '../../../data/UKDA-7724-csv/' # should match the path you unzipped the data to\n","df = pd.read_csv(datafolder + 'csv/eqls_2011.csv')\n","df_map = pd.read_csv(datafolder + 'mrdoc/excel/eqls_api_map.csv', encoding='latin1')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we are only interested in the UK for this example.\n","df = df.query('Y11_Country == 27')\n","\n","var_map = {\"Y11_Q42\": \"SRH\",\n","           'Y11_Deprindex': 'DeprIndex',\n","           \"Y11_Accommproblems\": 'AccomProblems',\n","           \"Y11_HHsize\": \"HouseholdSize\",\n","           \"Y11_Q32\": \"Children\",\n","           \"Y11_ISCEDsimple\":\"ISCED\",\n","           \"Y11_SocExIndex\":\"SocialExclusionIndex\",\n","           \"Y11_MWIndex\": \"MentalWellbeingIndex\",\n","           \"Y11_Agecategory\":\"AgeCategory\",\n","           \"Y11_HH2a\":\"Gender\",\n","           \"Y11_Q31\":\"MaritalStatus\",\n","           \"Y11_Country\":\"Country\"\n","}\n","\n","df.rename(columns=var_map, inplace=True)\n","df_set = df[var_map.values()]"]},{"cell_type":"markdown","metadata":{},"source":["We drop rows with missing data (**warning**: this shouldn't be done lightly without having explored the missingness of the data, here we are doing for simplicity and to focus on the modelling)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_model = df_set.dropna()"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we dichotomise the `SRH` variable."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dichotomise SRH\n","df_model['SRH_binary'] = df_model.SRH.apply(lambda x: 1 if float(x) <= 3 else 0)"]},{"cell_type":"markdown","metadata":{},"source":["## Model 1: Age and Deprivation index.\n","\n","Let's start evaluating the performance of a simple model described in [_Section 4.3_](section4.3) where we model `SRH` as a function of `Age` and `DeprIndex`. \n","\n","In that section we used our complete UK dataset to fit the model as an illustrative example. However, an alternative approach this is to partition your dataset into a **training** sample used to fit you model, and a **holdout** sample for evaluation. The purpose of this is to obtain an unbiased estimate of learning performance. \n","\n","Depending on your model you might need a **training**, **validation** and **testing** set (e.g a validation set can be useful when you have to tune model hyper-parameters). However, for this example we will use a simple **train**/**test** split following a 70/30 rule (for more in depth discussion of what is a \"good test size\" check-out this [blog post](https://www.r-bloggers.com/2021/01/what-is-a-good-test-set-size-2/))."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test train split using scikit learn, defining random state for reproducibility\n","trainX_model1, testX_model1, trainy_model1, testy_model1 = train_test_split(df_model[['AgeCategory','DeprIndex']], df_model.SRH_binary.values, test_size=0.3, random_state=2)"]},{"cell_type":"markdown","metadata":{},"source":["In this example we will use a fixed train/test split, but this has its dangers — what if the split we make isn’t random? (e.g data could be ordered in a non random manner, or we could be unlucky in our split to have a non-representative sample). A solution for this is to use **cross-validation**. This method is very similar to train/test split, but it’s applied to more subsets. Meaning, the dataset is split into $k$ subsets (or folds), and the model is trained on $k-1$ one of those subsets. The remaining subset is used to test the model. This is done iteratively $k$ times and then the score metrics obtained on each of the folds are averaged into a a summarized performance of the model. More information on **cross-validation** can be found [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n","\n","Now, let's fit our model on our training set."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["trainX_const_model1 = sm.add_constant(trainX_model1) #add constant for intercept\n","logit_model_model1 = sm.Logit(trainy_model1, trainX_const_model1) #Create model instance\n","result_model1 = logit_model_model1.fit() #Fit model"]},{"cell_type":"markdown","metadata":{},"source":["In the output, ‘Iterations‘ refer to the number of times the model iterates over the data to optimize the model."]},{"cell_type":"markdown","metadata":{},"source":["## Validating the fit\n","Let's explore the output of the fit. The summary table below gives us a descriptive summary about the results.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print (result_model1.summary())"]},{"cell_type":"markdown","metadata":{},"source":["In [_Section 4.3_](section4.3) we discussed the interpretation of the coefficients, standard errors and significance parameters. In this section we'll explore the output related to the **goodness-of-fit**.\n","\n","- **Method**:  Maximum Likelihood Estimation (MLE) is a probabilistic framework for estimating the parameters of an assumed probability distribution, given some observed data. MLE involves maximizing a likelihood function in order to find the probability distribution and parameters that best explain the observed data.\n","- **No. Observations**: Number of observations on our training set.\n","- **Df. Residuals**:  This is the Residuals degrees of freedom in the model. This is calculated in the form of `number of observations` - `number of predictors` - 1.\n","- **Df. Model**: Degrees of Freedom of our model, which is basically the number of predictors used in the model (or number of coefficients to be fitted).\n","- **Log-Likelihood**: the natural logarithm of the Maximum Likelihood Estimation (MLE) function given the estimated parameters. The log likelihood function in maximum likelihood estimations is usually computationally simpler, and the log-likelihood maximum is the same as the maximum likelihood.\n","- **LL-Null**: the value of log-likelihood of the model when no independent variable is included (only an intercept is included).\n","- **Pseudo R-squ.**: It is the ratio of the log-likelihood of the null model to that of the full model (this can function as a substitute for the R-squared value in Least Squares linear regression).\n","- **LLR p-value**: A small p-value you can reject the null hypothesis that the model based on the intercept (all coefficients = 0) is better than the full model, again this uses the ratio of the log-likelihood of the null model to that of the full model.\n","\n","For the remainder of this section we will be using the **log-likelihood** values as a way to compare how a model improves when adding a new predictor. "]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation through predicting new data\n","  \n","Now we evaluate our model using our test dataset. \n","\n","As we learned in [_Section 4.2_](section4.2), logistic regression works by first obtaining the prediction of the model as a probability, then binarising the predicted probability using a threshold value. Scores above the threshold value will be classified as positive, those below as negative. For now the threshold value of the probability is $P(x)>0.5$."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# performing predictions on the test dataset\n","yhat = result_model1.predict(sm.add_constant(testX_model1))\n","pred_y_model1 = list(map(round, yhat))\n"," \n","# comparing first 10 original and predicted values of y\n","print('Actual values', list(testy_model1)[:10])\n","print('Predictions :', pred_y_model1[:10])"]},{"cell_type":"markdown","metadata":{},"source":["In the cell above we are just printing the first 10 observations of our dataset. In order to summarise the the accuracy of the predictions from our model we can use a confusion matrix. This maps the predicted and actual values into a matrix and can give us an idea of what the classification model is getting right and what types of errors it is making.\n","\n","From a confusion matrix we can obtain the True Positives (TP), False Positives (FP), True Negatives (TN) and False Negatives (FN) and calculate the following metrics:\n","\n","\n","- $\\text{Accuracy} = \\frac{TP  + TN}{TP + FP + TN + FN}$\n","\n","\n","- $\\text{Precision} = \n","     \\frac{TP}{TP + FP}$\n","     \n","     \n","- $\\text{Recall (Sensitivity)} =\n","      \\frac{TP}{TP + FN}$\n","\n","- ${\\text{Specificity (Recall of Negative label)}} =\n","      \\frac{TN}{TN + FP}$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# confusion matrix using sklearn\n","cnf_matrix = metrics.confusion_matrix(testy_model1,pred_y_model1)\n","\n","def plt_cnf_mat(cnf_matrix, ax, class_names=[0,1]):\n","    tick_marks = np.arange(len(class_names))\n","    ax.set_xticks(tick_marks)\n","    ax.set_xticklabels(class_names)\n","    ax.set_yticks(tick_marks)\n","    ax.set_yticklabels(class_names)\n","    # create heatmap\n","    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n","    ax.set_title('Confusion matrix', y=1.1)\n","    ax.set_ylabel('Actual label')\n","    ax.set_xlabel('Predicted label')\n","    \n","    \n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","plt_cnf_mat(cnf_matrix, ax)\n","\n","\n","print(\"Accuracy:\",metrics.accuracy_score(testy_model1, pred_y_model1))\n","print(\"Precision:\",metrics.precision_score(testy_model1, pred_y_model1))\n","print(\"Recall:\",metrics.recall_score(testy_model1, pred_y_model1)) # what is recall? "]},{"cell_type":"markdown","metadata":{},"source":["You can see here that very few true negative values get predicted and the overwhelming majority of responses are true positives. Furthermore, the accuracy, precision, and recall score > 89%, this is happening due to our dataset being highly imbalanced and that the minority class are labeled as negative (0).\n","\n","A more complete evaluation would be to also estimate these metrics for the minority class (specificity):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testy_model1_minority = abs(testy_model1 - 1)\n","pred_y_model1_minority = abs(np.array(pred_y_model1) -1)\n","\n","print(\"Specificity:\",metrics.recall_score(testy_model1_minority, pred_y_model1_minority))"]},{"cell_type":"markdown","metadata":{},"source":["The result above is not a surprise. In [_Section 4.3_](section4.3) we saw that the model considers deprivation as a continuous variable and observed that the cutoff point of the model (this is the value where the model starts predicting negative labels) for the deprivation predictor was around 7, which is larger than the existing range for this variable. Nonetheless, in our examination of the model in [_Section 4.3_](section4.3) we observed that the model was indeed learning from the data and reproducing it, and after examining the **log-likelihoods** of the fit summary above we can conclude that the model is indeed better than a null model where the predictors do not add any information.\n","\n","This means that the classification matrix  might not be the best tool to evaluate the model in this scenario. Particularly because it shows a snapshot of the results being mapped from a predicted probability using a threshold of $p(x)> 0.5$.\n","\n","\n","### Investigating threshold values\n","\n","Let's go back to our probability and investigate the predicted values given by the model for our two labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["# link the prediction to the label values\n","df_labels = pd.DataFrame({'prediction': yhat,\n","     'label': testy_model1})\n","\n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","df_labels[df_labels['label']==1]['prediction'].plot.kde(label='SRH == 1')\n","df_labels[df_labels['label']==0]['prediction'].plot.kde(label='SRH == 0')\n","pyplot.xlabel('p(x)')\n","pyplot.legend()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Ideally, in this density plot we would observe separate probability scores between two classes, with the score of the cases where `SRH==0` on the low probability values and the score of cases with `SRH==1` on the the high values. However, in the current case both distributions are skewed to the high probability values. The reason for this is because our dataset is highly imbalanced and only consists of 10 percent of cases where `SRH ==0`. Thus the predicted probabilities get pulled towards higher values because of the majority of the data being positive cases."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print ('Mean value for SRH ==1,', round(df_labels[df_labels['label']==1]['prediction'].mean(),3))\n","print ('Mean value for SRH ==0,', round(df_labels[df_labels['label']==0]['prediction'].mean(),3))\n","print ('Percentage of cases with SRH ==0,', round(df_labels[df_labels['label']==0]['prediction'].count()/df_labels.shape[0],3))"]},{"cell_type":"markdown","metadata":{},"source":["Despite all this, in the density figures above we can observe a separation in the predicted scores, and the performance of the model could be improved by selecting a better threshold value for $p(x)$ to classify each case. This should be done by balancing the rate of False Positives and False Negatives. To assess this trade-off we can use other tools available, such as **ROC curves**."]},{"cell_type":"markdown","metadata":{},"source":["### What Are ROC Curves?\n","\n","A receiver operating characteristic curve, or ROC curve, is a figure that illustrates the performance of a binary classifier as its discrimination threshold is varied.\n","\n","Let's examine how the ROC curve looks for our model, and how it compares to a dummy classifier with no skill in classifying our data. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# function from https://stackoverflow.com/questions/22518230/creating-a-threshold-coded-roc-plot-in-python\n","def plot_roc(labels, predictions, positive_label, thresholds_every=10, title='', c='darkorange'):\n","    # fp: false positive rates. tp: true positive rates\n","    fp, tp, thresholds = roc_curve(labels, predictions, pos_label=positive_label)\n","    roc_auc = roc_auc_score(labels, predictions)\n","\n","    plt.plot(fp, tp, label=title+' ROC curve (area = %0.2f)' % roc_auc, linewidth=2, color=c)\n","    plt.plot([0, 1], [0, 1], color='black', linestyle='--', linewidth=2)\n","    plt.xlabel('False positives rate')\n","    plt.ylabel('True positives rate')\n","    plt.xlim([-0.03, 1.0])\n","    plt.ylim([0.0, 1.03])\n","    plt.title('ROC curve (numbers are threshold values)')\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(True)\n","\n","    # plot some thresholds\n","    thresholdsLength = len(thresholds)\n","    for i in range(0, thresholdsLength, thresholds_every):\n","        threshold_value_with_max_three_decimals = str(thresholds[i])[:4]\n","        plt.text(fp[i] - 0.05, tp[i] + 0.015, threshold_value_with_max_three_decimals, fontdict={'size': 10},color=c)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr_probs_model1 = result_model1.predict(sm.add_constant(testX_model1))\n","\n","plot_roc(testy_model1, lr_probs_model1, positive_label=1, thresholds_every=3, title=\"Model 1\",c='blue')"]},{"cell_type":"markdown","metadata":{},"source":["The AUC values printed above reflects the area under the ROC curve and provides a measure of how discriminant the model is between the two classes.  [In general, an AUC of 0.5 means no discrimination, 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding](https://www.sciencedirect.com/science/article/pii/S1556086415306043#:~:text=AREA%20UNDER%20THE%20ROC%20CURVE,-AUC%20is%20an&text=In%20general%2C%20an%20AUC%20of,than%200.9%20is%20considered%20outstanding.).\n","\n","A value of 0.5 for AUC indicates that the ROC curve will fall on the diagonal (i.e., 45-degree line) and hence suggests that the diagnostic test has no discriminatory ability. \n","\n","Here we confirm again that our model has learned something and performs better than an \"unskilled\" dummy classifier. Now, we can use the ROC values to obtain an optimal threshold value. One of the commonly used method for this is the [Youden index method](https://en.wikipedia.org/wiki/Youden%27s_J_statistic). In this method the optimal threshold values is the one that maximises the Youden function which is the difference between true positive rate and false positive rate over all possible threshold values.\n","\n","We wrote a small implementation of this method for this example:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# function inspired by https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python\n","def find_optimal_threshold(target, predicted):\n","    \"\"\" Find the optimal probability threshold for a classification model.\n","    Parameters\n","    ----------\n","    target : Matrix with label data, where rows are observations\n","\n","    predicted : Matrix with predicted data, where rows are observations\n","\n","    Returns\n","    -------     \n","    a float, with optimal cutoff value\n","        \n","    \"\"\"\n","    fpr, tpr, thresholds = roc_curve(target, predicted)\n","    j_scores = tpr-fpr\n","    j_ordered = sorted(zip(j_scores,thresholds))\n","    return j_ordered[-1][1]\n","\n","threshold = find_optimal_threshold(df_labels['label'], df_labels['prediction'])\n","print (round(threshold,4))"]},{"cell_type":"markdown","metadata":{},"source":["This is quite far from the 0.5 value we originally had! Let's see how our classification matrix does now using this new threshold."]},{"cell_type":"markdown","metadata":{},"source":["In this case we have decided that the optimal threshold value is the one that, which which maximises the True Positive rate and minimizes the False Positive rate. However, the definition of **optimal** really depends of the research question or the task we are to solve. An alternative would be to give more importance in accurately classifying our `SRH=0` class and try to maximise the True Negative Rate. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# performing predictions on the test dataset\n","yhat = result_model1.predict(sm.add_constant(testX_model1))\n","pred_y_model1 =  [1 if x > threshold else 0 for x in yhat]\n","\n","\n","# confusion matrix using sklearn\n","cnf_matrix = metrics.confusion_matrix(testy_model1,pred_y_model1)\n","    \n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","plt_cnf_mat(cnf_matrix, ax)\n","\n","\n","print(\"Accuracy:\",metrics.accuracy_score(testy_model1, pred_y_model1))\n","\n","print ()\n","print(\"Precision:\",metrics.precision_score(testy_model1, pred_y_model1))\n","print(\"Recall:\",metrics.recall_score(testy_model1, pred_y_model1)) # what is recall? \n","\n","print ()\n","testy_model1_minority = abs(testy_model1 - 1)\n","pred_y_model1_minority = abs(np.array(pred_y_model1) -1)\n","\n","print(\"Specificity:\",metrics.recall_score(testy_model1_minority, pred_y_model1_minority))"]},{"cell_type":"markdown","metadata":{},"source":["The specificity for our minority label (`SRH==0`) has improved using the new threshold value, at a large expense of the classification performance of the majority label (`SRH==1`). However, a model that puts all instances in one class is not of good use for us, so this is an improvement. Furthermore, depending on our research question and the ultimate goal of the model we might want to maximise the specificity, even if this means increasing the amount of false negatives we observe. For example, it might be more important to identify people that are likely to report poor health than the ones with good health.\n","\n","In any case, we must remember that up to know we are using a very simple model, only using deprivation as a predictor and controlling for age. Adding more predictors to the model can improve its performance. We'll explore that next."]},{"cell_type":"markdown","metadata":{},"source":["## Model 2: Model 1 + Education, Children, and Accommodation Problems\n","\n","Let's increase the complexity to our model by adding more predictors representing socio-economic factors. In this case we'll add information about education (`ISCED`; levels of education completed), the number of children one has (`Children`), and the number of problems with accommodation reported by the respondents (`AccomProblems`). All these variables are ordered categorical variables that we assume to function as continuous variables for this exercise."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainX_model2, testX_model2, trainy_model2, testy_model2 = train_test_split(df_model[['AgeCategory','DeprIndex','ISCED',\"Children\",\"AccomProblems\"]], df_model.SRH_binary.values, test_size=0.3, random_state=2)\n","\n","trainX_const_model2 = sm.add_constant(trainX_model2) #add constant for intercept\n","logit_model2 = sm.Logit(trainy_model2, trainX_const_model2) #Create model instance\n","result_model2 = logit_model2.fit() #Fit model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print (result_model2.summary())"]},{"cell_type":"markdown","metadata":{},"source":["If we compare these fit results to the ones from **Model 1** we observe the following:\n","\n","- The coefficient for `DeprIndex` has become less negative. This could mean that the new variables that we have added to the model have taken some weight previously given to `DeprIndex` and `Age`. This is not surprising given that somese variables are expected to be linked to deprivation (e.g in [_Section 3.5_](section3.5) we observed a correlation between `DeprIndex` and `AccomProblems`).\n","\n","- The coefficient for `AgeCategory` also became less negative, but within the margin of its standard error.\n","\n","- Now, if we look at the parameters of *goodness-of-fit* we can observe that the fit has improved slightly, with a less negative log-likelihood and slightly larger pseudo R-squ. However, if we compare the increase of log-likelihood of adding these new variables with respect to the values of a null model, the improvement is modest (~10 units of log likelihood of **Model 2** vs **Model 1** against ~50 units for **Model 1** vs **Null model**). We might suspect that our new variables barely increase the discrimination power of our model. Lets take a look at evaluating it on the test set."]},{"cell_type":"markdown","metadata":{},"source":["### Comparing predicted values \n","\n","Same as we did in the previous section, let's compare the predicted probability values of an event being classified as `SRH=1` or `SRH=0`. Let's do this but running the model on the test set and comparing the distributions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# link the prediction to the label values\n","yhat_model2 = result_model2.predict(sm.add_constant(testX_model2))\n","df_labels = pd.DataFrame({'prediction': yhat_model2,\n","     'label': testy_model2})\n","\n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","df_labels[df_labels['label']==1]['prediction'].plot.kde(label='SRH == 1')\n","df_labels[df_labels['label']==0]['prediction'].plot.kde(label='SRH == 0')\n","pyplot.xlabel('Probability')\n","pyplot.legend()"]},{"cell_type":"markdown","metadata":{},"source":["The shape of the probability distributions look similar to what we observed in **Model 1** (maybe the tails for class `SHR=0` are longer). Again, we can observe that using the default threshold value around 0.5 will not be optimal in this case and a higher value should be chosen.\n","\n","Probability values of larger than one in this case are a product of using kernel density estimation for our visualisation. We choose this method because we want to compare shapes of distributions, however we must have in mind that it can create the impression that there is data in ranges where there is not.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["threshold_model2 = find_optimal_threshold(testy_model2, yhat_model2)\n","print ('Probability threshold cut',round(threshold_model2,4))\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["### ROC curves\n","The probability threshold value is slightly lower than for model **Model 1**, but not significantly, given that the predicted distributions are quite similar.\n","\n","Now, let's take a look at our ROC curves and AUC values and compare them to **Model 1**."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_roc(testy_model1, lr_probs_model1, positive_label=1, thresholds_every=5, title=\"Model 1\",c='blue')\n","plot_roc(testy_model2 , yhat_model2, positive_label=1, thresholds_every=15, title=\"Model 2\",c='red')"]},{"cell_type":"markdown","metadata":{},"source":["When comparing the above curves between the two models we can observe the following:\n","\n","- The ROC curve for **Model 2** has a higher granularity of steps in which the True and False Positive rates are evaluated. This is because the sklearn function drops *suboptimal thresholds* which corresponds points on the ROC curve that will have the same TPR and FPR values that adjacent points. In the simpler model there might be more configuration of predictor values needed to produce a high granularity curve.\n","\n","- Despite the point above, AUC values and overall ROC shapes are pretty much the same.\n","\n","This again confirms our theory that this new model doesn't contain much more discriminative power than **Model 1**, with the deprivation index carrying a good bulk of the information that these new variables such as education level and accommodation problems can add to the model."]},{"cell_type":"markdown","metadata":{},"source":["### Classification matrix\n","\n","Finally, let's take a look at our classification performance, using the optimal threshold value of classification for this model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_y_model2 =  [1 if x > threshold_model2 else 0 for x in yhat_model2]\n","\n","cnf_matrix_model2  = metrics.confusion_matrix(testy_model2,pred_y_model2)\n","\n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","plt_cnf_mat(cnf_matrix_model2 , ax)\n","\n","\n","print(\"Accuracy:\",metrics.accuracy_score(testy_model2 , pred_y_model2 ))\n","\n","print()\n","print(\"Precision:\",metrics.precision_score(testy_model2 , pred_y_model2 ))\n","print(\"Recall:\",metrics.recall_score(testy_model2 , pred_y_model2 )) \n","\n","\n","print ()\n","testy_model2_minority = abs(testy_model2 - 1)\n","pred_y_model2_minority = abs(np.array(pred_y_model2) -1)\n","\n","print(\"Specificity:\",metrics.recall_score(testy_model2_minority, pred_y_model2_minority))"]},{"cell_type":"markdown","metadata":{},"source":["Comparing to **Model 1**:\n","    \n","- Our general accuracy improved slightly (from 0.55 to 0.58!).\n","- The Recall decreased (from 0.52 to 0.55).\n","- Specificity decreased slightly.\n","\n","It can be difficult to track the changes of all of these metrics but with such an imbalanced dataset **Accuracy** is not a good indicator. Particularly, in this case we might want to minimize False Negatives. For these cases, we can use the **F1-score**, which is is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the **Accuracy** metric.\n","\n","$F1  = 2*\\frac{Precision*Recall}{Precision+ Recall}$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print ('Model 1:', metrics.f1_score(testy_model1 , pred_y_model1))\n","print ('Model 2:', metrics.f1_score(testy_model2 , pred_y_model2))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model 3: Model 2 + Mental Wellbeing \n","\n","Now let's complicate the model even further, adding a variable related to mental wellbeing.\n","\n","The mental wellbeing index is based on a set of five questions where the respondents state degree of agreement in how they felt over the previous two weeks, measured on a six-point scale. The mental well-being scale converts these to a value between 0 - 100.\n","The questionnaire use only positively phrased questions and in the scale higher values mean better self reported mental health.\n","\n","We decided to add this variable because they should be mostly orthogonal to the existing ones of **Model 2**, and a new dimension to the model that should improve its performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df_model[['AgeCategory','DeprIndex','ISCED',\"MentalWellbeingIndex\",\"Children\",\"AccomProblems\"]]\n","\n","y = df_model.SRH_binary.values\n","\n","trainX_model3, testX_model3, trainy_model3, testy_model3 = train_test_split(X, y, test_size=0.3, random_state=2)\n","trainX_const_model3 = sm.add_constant(trainX_model3) #add constant for intercept\n","logit_model3 = sm.Logit(trainy_model3, trainX_const_model3) #Create model instance\n","result_model3 = logit_model3.fit() #Fit model\n","\n","print (result_model3.summary())"]},{"cell_type":"markdown","metadata":{},"source":["Let's repeat the exercise we did in the previous section and compare the fit results between **Model 2** and **Model 3**. We observe the following:\n","\n","- The coefficient for `DeprIndex` again has become significantly less negative. This could mean that the mental wellbeing variables is sharing the weight that the deprivation index carried (it is not a surprise to think that people living in deprivation are also likely to have poor mental health).\n","- The coefficient for `MentalWellbeingIndex` is one order of magnitude smaller than the other coefficients. This is expected\n","given that the variable is one order of magnitude larger.\n","\n","\n","\n","- Now, if we look at the parameters of *goodness-of-fit* we can observe that the fit has improved significantly, with a less negative log-likelihood (~60 units of log likelihood of **Model 3** vs **Model 2**) and larger pseudo R-squ (~0.25 vs ~0.13). In this case the improvement is comparable to the increase of log-likelihood of adding new variables in **Model 2** with respect to the values of a null model (~60 units of log likelihood of **Model 3** vs **Model 2** against 63 units for **Model 2** vs **Null model**).\n","\n","Take this into consideration, we could suspect that including `MentalWellbeingIndex` will increase the discrimination power of our model. Let's see if this is the case by evaluating it on the test set."]},{"cell_type":"markdown","metadata":{},"source":["## Comparing predicted values\n","Same as we did in the previous section, let's compare the predicted probability values of an event being classified as `SRH=1` or `SRH=0`. Let's do this but running the model on the test set and comparing the distributions.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# link the prediction to the label values\n","yhat_model3 = result_model3.predict(sm.add_constant(testX_model3))\n","df_labels = pd.DataFrame({'prediction': yhat_model3,\n","     'label': testy_model3})\n","\n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","df_labels[df_labels['label']==1]['prediction'].plot.kde(label='SRH == 1')\n","df_labels[df_labels['label']==0]['prediction'].plot.kde(label='SRH == 0')\n","pyplot.xlabel('Probability')\n","pyplot.legend()"]},{"cell_type":"markdown","metadata":{},"source":["The shape of for our class `SHR=0` has indeed changed, with now even larger tails to low probability values whilst the shape of our positive class has stayed more or less the same. This points in the same direction of our assumption that **Model 3** has now more discrimination power.\n","\n","Let's confirm this using the **ROC** curves."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr_probs_model3 = result_model3.predict(sm.add_constant(testX_model3))\n","\n","plot_roc(testy_model1, lr_probs_model1, positive_label=1, thresholds_every=5, title=\"Model 1\",c='blue')\n","plot_roc(testy_model2 , yhat_model2, positive_label=1, thresholds_every=20, title=\"Model 2\",c='red')\n","plot_roc(testy_model3 , lr_probs_model3, positive_label=1, thresholds_every=20, title=\"Model 3\")"]},{"cell_type":"markdown","metadata":{},"source":["Indeed, **Model 3** appears to be the one with better discrimination power between all models from this section. Now, finally let's look at our classification performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# performing predictions on the test dataset\n","threshold_model3 = find_optimal_threshold(testy_model3, yhat_model3)\n","print ('p(x) threshold value', round(threshold_model3,4))\n","\n","pred_y_model3 =  [1 if x > threshold_model3 else 0 for x in yhat_model3]\n","\n","\n","cnf_matrix_model3  = metrics.confusion_matrix(testy_model2,pred_y_model3)\n","\n","fig, ax = plt.subplots(1,1, figsize = (5,5))\n","plt_cnf_mat(cnf_matrix_model3 , ax)\n","\n","\n","print(\"Accuracy:\",metrics.accuracy_score(testy_model3 , pred_y_model3 ))\n","\n","print()\n","print(\"Precision:\",metrics.precision_score(testy_model3 , pred_y_model3 ))\n","print(\"Recall:\",metrics.recall_score(testy_model3 , pred_y_model3 )) \n","\n","\n","print ()\n","testy_model3_minority = abs(testy_model3 - 1)\n","pred_y_model3_minority = abs(np.array(pred_y_model3) -1)\n","\n","print(\"Specificity:\",metrics.recall_score(testy_model3_minority, pred_y_model3_minority))\n","\n","print ('F-score Model 3:', metrics.f1_score(testy_model3 , pred_y_model3))"]},{"cell_type":"markdown","metadata":{},"source":["Most of our metrics have improved with respect to **Model 2**!"]},{"cell_type":"markdown","metadata":{},"source":["### Going back to our research question\n","\n","Up to now we have been exploring different models and attempting to contrast and compare them using different metrics following a typical machine learning approach. However, we need to sit back and think about our research question what exactly we are trying to measure:\n","\n","> We want to investigate the contribution of material, occupational, and psychosocial factors on the self reported health (SRH) across different European countries. We will use SRH information collected by the Wave 2 and 3 of the EQLTS survey, aware that they offer only a partial representation of European populations and that SRH is per-se a highly subjective indicator, difficult to compare across countries. \n","\n","Our goal here is not necessarily to accurately classify between the two classes but to measure the level of contribution that these predictors have in our model (hence improving the discrimination power of the model and therefore contributing to accurately classifying between the classes). Some of the metrics we have been using in this section (log-likelihood and AUC) can definitely help us measure these contributions.\n","\n","For example, in **Model 3** just adding the mental wellbeing predictor into the model significantly improved the discrimination power of the model for all of our different metrics, therefore we could hint that the self reported mental wellbeing has a sizable contribution to the self reported health for the UK.\n","\n","Up to now we have been comparing the log-likehoods between our different models. The reason we can do this is because these are **nested models** meaning that one model is a special case of the other (e.g Model 1 contains a subset of the predictors of Model 2, and equivalent for Model 2 and 3). We haven't been explicit about this but what we are going is equivalent to a likelihood ratio test, which is a statistical test to determine if one (more complex) model fits the data significantly better than the other. The Likelihood (L) ratio is based on the statistic:\n","\n","$\\lambda = -2 ln \\left(\\frac{{L}(ModelA)}{{L}(ModelB)}\\right)= 2 \\left(log{L}(ModelA) - log{L}(ModelB)\\right)$\n","\n","In the null model scenario (in this case that Model B is not better at fitting the data than Model A), the test statistic follows a ${\\chi}^2$ distribution with degrees of freedom, k, equal to the difference in the number of parameters between the two models being fitted.  This is known as [Wilk’s theorem](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-9/issue-1/The-Large-Sample-Distribution-of-the-Likelihood-Ratio-for-Testing/10.1214/aoms/1177732360.full).\n","\n","Knowing this, we can calculate a p-value using the chi-square test using the cumulative density function of the ${\\chi}^2$ distribution :\n","\n","$p=1-cdf\\chi^{2}(\\lambda,k)$  \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["So, lets calculate the likelihood ratio of Model 3 against Model 2. Given that we have both log-likelihoods, calculating the test statistic is simple:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LR = 2 *(result_model3.llf - result_model2.llf)\n","\n","print (LR)"]},{"cell_type":"markdown","metadata":{},"source":["So our likelihood ratio test statistic is around 120, with one degree of freedom, given that we only added one extra variable to Model 3. We can now calculate the p-value:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["degrees_of_freedom = 1\n","p = 1 - stats.chi2.cdf(LR,1)\n","\n","print (p)"]},{"cell_type":"markdown","metadata":{},"source":["Such a small p-value confirms what we already know that Model 3 fits significantly better the data than the Model 2."]},{"cell_type":"markdown","metadata":{},"source":["### Model 4: A simpler model with equivalent discrimination power\n","\n","During this section we have slowly built the complexity of the model in order to improve the discrimination power of it. However, we don't necessarily always want a very complex model, given that this can lead to problems with model explanability and **overfitting**. Furthermore, in this case we can to assess the contribution of individual SE factors into the self reported health, and ideally we would only use variables that are completely orthogonal to each other (i.e., measure different things). However, we don't want to lose model performance by reducing the amount of information we feed into it.\n","\n","Everything we have learn about our data by slowly increasing the complexity of the models hint us that is possible to build a simpler model that is equivalent in discrimination power. Let's see what happens if we remove the variables  `Children` and `AccomProblems` that we know are related to `DeprIndex` and `Age`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df_model[['AgeCategory','DeprIndex','ISCED',\"MentalWellbeingIndex\"]]\n","\n","y = df_model.SRH_binary.values\n","\n","trainX_model4, testX_model4, trainy_model4, testy_model4 = train_test_split(X, y, test_size=0.3, random_state=2)\n","trainX_const_model4 = sm.add_constant(trainX_model4) #add constant for intercept\n","logit_model4 = sm.Logit(trainy_model4, trainX_const_model4) #Create model instance\n","result_model4 = logit_model4.fit() #Fit model\n","\n","print (result_model4.summary())"]},{"cell_type":"markdown","metadata":{},"source":["Only 4 units of **log-likelihood** are lost by dropping these variables. And the **Pseudo R-squ.** is equivalent."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr_probs_model4 = result_model4.predict(sm.add_constant(testX_model4))\n","\n","plot_roc(testy_model3 , lr_probs_model3, positive_label=1, thresholds_every=20, title=\"Model 3\",c='red')\n","plot_roc(testy_model4 , lr_probs_model4, positive_label=1, thresholds_every=20, title=\"Model 4\")"]},{"cell_type":"markdown","metadata":{},"source":["Our **ROC** curves are equivalent!\n"]},{"cell_type":"markdown","metadata":{},"source":["### References and Further Reading\n","\n","[Blog post: Testing if one model fits the data significantly better than another model](http://sherrytowers.com/2019/03/18/determining-which-model-fits-the-data-significantly-better/)\n","\n","[Blog post: How are the likelihood ratio, wald and lagrange multiplier test different and/or similar](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('playground')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"bfb6be6a1e2928ff2d2ab7904db3c9037c4b5ca4f399bd5e7064365a54a3f091"}}},"nbformat":4,"nbformat_minor":1}
