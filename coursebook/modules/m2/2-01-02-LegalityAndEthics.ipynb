{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 Legality and Ethics\n",
    "\n",
    "Before we dive into using any dataset, we need to consider who **owns**\n",
    "the data that we wish to use and what **restrictions** they or\n",
    "regulatory/governing bodies may have put in place. We must also consider\n",
    "the ethical implications of using the dataset.\n",
    "\n",
    "## Legality\n",
    "\n",
    "**Important disclaimer: we are not legal experts, and this is not legal\n",
    "advice. The content in this section is non-exhaustive, based on our\n",
    "personal experiences, and is aimed at making you aware of these issues.\n",
    "Check your university/employer’s policies for guidance. If in any doubt,\n",
    "speak to an expert.**\n",
    "\n",
    "### Licences\n",
    "\n",
    "> A licence agreement is a legal arrangement between the\n",
    "> creator/depositor of the data set and the data repository, signifying\n",
    "> what a user is allowed to do with the data.\n",
    "\n",
    "— *[The Data Management Expert Guide by CESSDA\n",
    "ERI](https://www.cessda.eu/Training/Training-Resources/Library/Data-Management-Expert-Guide)*\n",
    "\n",
    "As a user, it is your responsibility to abide by the terms of a licence\n",
    "when making use of a dataset. This is similar to licensing for software.\n",
    "\n",
    "\\`\\`\\`{margin} Open Data \\> Open data is data that can be freely used,\n",
    "re-used and redistributed by anyone – subject only, at most, to the\n",
    "requirement to attribute and sharealike.\n",
    "\n",
    "— [Open Data\n",
    "Handbook](http://opendatahandbook.org/guide/en/what-is-open-data/)\n",
    "\n",
    "\n",
    "    Licences range in the freedoms they give to users. Licences that grant users freedoms over (re)use and (re)distribution are *permissive*. Some common examples are:\n",
    "\n",
    "     - [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)\n",
    "         - the dataset may be shared and adapted as long as credit is given, the original is linked, and any changes are noted.\n",
    "     - [CDLA-Sharing-1.0](https://cdla.io/sharing-1-0/)\n",
    "         - a \"copy-left\" license. Similar restrictions to CC BY 4.0 but you must use the same license if resharing the data (or \"enhanced data\"). Importantly, restrictions **do not** apply to results of computational use.\n",
    "\n",
    "\n",
    "    However, not all licences give so many freedoms. It's always worth checking.\n",
    "\n",
    "    ```{warning}\n",
    "    All open source/open access licensed materials are provided as-is. Use with caution.\n",
    "\n",
    "`{admonition} Research vs non-Research Some licenses will grant permission for *research only*. There is a blurred line between what may constitute research vs non-research. We suggest that you check internally with your organisation. This determination can change depending on the nature of the work; this is especially important to remember if your time is split over multiple projects.`\n",
    "\n",
    "**NB**: GitHub’s default stance for when there is [no\n",
    "licence](https://choosealicense.com/no-permission) present is that the\n",
    "work is under exclusive copyright.\n",
    "\n",
    "More information on data licences can be found in [The Turing\n",
    "Way](https://the-turing-way.netlify.app/reproducible-research/licensing/licensing-data.html).\n",
    "\n",
    "### Data Protection & GDPR\n",
    "\n",
    "> The Data Protection Act 2018 is the UK’s implementation of the General\n",
    "> Data Protection Regulation (GDPR).\n",
    ">\n",
    "> Everyone responsible for using personal data has to follow strict\n",
    "> rules called ‘data protection principles’. They must make sure the\n",
    "> information is:\n",
    ">\n",
    "> -   used fairly, lawfully and transparently\n",
    "> -   used for specified, explicit purposes\n",
    "> -   used in a way that is adequate, relevant, and limited to only what\n",
    ">     is necessarily accurate and, where necessary, kept up to date\n",
    "> -   kept for no longer than is necessary\n",
    "> -   handled in a way that ensures appropriate security, including\n",
    ">     protection against unlawful or unauthorised processing, access,\n",
    ">     loss, destruction, or damage\n",
    ">\n",
    "> There is stronger legal protection for more sensitive information,\n",
    "> such as:\n",
    ">\n",
    "> -   race\n",
    "> -   ethnic background\n",
    "> -   political opinions\n",
    "> -   religious beliefs\n",
    "> -   trade union membership\n",
    "> -   genetics\n",
    "> -   biometrics (where used for identification)\n",
    "> -   physical or mental health\n",
    "> -   philosophical beliefs\n",
    "> -   sex life or orientation\n",
    ">\n",
    "> There are separate safeguards for personal data relating to criminal\n",
    "> convictions and offences.\n",
    "\n",
    "— *excerpt from https://www.gov.uk/data-protection*\n",
    "\n",
    "Note: The DPA 2018 sits alongside and supplements [UK\n",
    "GDPR](https://www.legislation.gov.uk/eur/2016/679/contents).\n",
    "\n",
    "#### What’s personal data?\n",
    "\n",
    "Personal data is information about a particular living individual. It\n",
    "doesn’t need to be “private” information.\n",
    "\n",
    "Truly anonymous information isn’t covered - but if you could use the\n",
    "data to identify someone (e.g., by combining it with another source) it\n",
    "still counts as personal data.\n",
    "\n",
    "Summarised from:\n",
    "https://ico.org.uk/for-organisations/guide-to-data-protection/introduction-to-data-protection/some-basic-concepts/\n",
    "\n",
    "See more at:\n",
    "https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/key-definitions/what-is-personal-data/\n",
    "\n",
    "#### Does it apply to me?\n",
    "\n",
    "> The law applies to any ‘processing of personal data’, and will catch\n",
    "> most businesses and organisations, whatever their size.\n",
    "\n",
    "— *excerpt from [ICO Guide to Data\n",
    "Protection](https://ico.org.uk/for-organisations/guide-to-data-protection/introduction-to-data-protection/some-basic-concepts/?q=apply+to+me#2)*\n",
    "\n",
    "However, there are a few exemptions to GDPR and some things that are\n",
    "simply outside its scope.\n",
    "\n",
    "Personal or household activities are considered outside the scope of the\n",
    "UK’s GDPR. Data **only** for personal use won’t make you subject to\n",
    "GDPR.\n",
    "\n",
    "Crucially, for academia and research, there are some exemptions. The\n",
    "details of these can be complicated and are certainly beyond the scope\n",
    "of this course. Each organisation will have their own stance on GDPR,\n",
    "and you should consult internally to determine how to proceed.\n",
    "\n",
    "For exemptions, see:\n",
    "https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/exemptions/\n",
    "\n",
    "### Commercial agreements\n",
    "\n",
    "Research projects are often collaborative efforts that may span multiple\n",
    "organisations. Commonly, these collaborations take place under some\n",
    "defined commercial agreement (a contract). Such an agreement may impact\n",
    "on your use of data.\n",
    "\n",
    "For example:\n",
    "\n",
    "*VacuumOrg have a contract with your organisation, DullResearchInc, to\n",
    "conduct research into consumer spending on robot vacuum cleaners. As\n",
    "part of this research, MarketingOrg have shared some survey data with\n",
    "you that they have collected over the last 6 months. The data collection\n",
    "was expensive, and the data is considered hot property by MarketingOrg;\n",
    "they don’t wish their competitors to have access to it and only share it\n",
    "with you under a Non-Disclosure Agreement (NDA).*\n",
    "\n",
    "*You want to transfer the data (5Mb) from your colleagues PC to your\n",
    "work laptop. Assuming some oddly limited options, which of the following\n",
    "do you do?* - *A. upload it to your public GitHub repository so you can\n",
    "pull it down later* - *B. transfer via a DullResearchInc USB stick*\n",
    "\n",
    "## Ethics\n",
    "\n",
    "**Ethics are a shared responsibility, however, it’s another area that we\n",
    "should all feel comfortable seeking expert advice. At Turing, we have an\n",
    "Ethics Advisory Board that we can look to for guidance.**\n",
    "\n",
    "Although there will be a lot of crossovers between legality and ethics,\n",
    "we should be aware that there may be additional ethical considerations\n",
    "over use of a dataset, even when its use is deemed legal.\n",
    "\n",
    "For example:\n",
    "\n",
    "\\*The UK data service lists the European Quality of Life Time Series,\n",
    "2007 and 2011: Open Access dataset available for download and use under\n",
    "a CC BY 4.0 licence. As the data do not contain direct personal\n",
    "identifiers, and it is unlikely that anyone will be able to identify\n",
    "individuals, we are confident that, from a data protection angle, we can\n",
    "proceed.\n",
    "\n",
    "*However, we may wish to consider the consent under which this data was\n",
    "originally gathered. Did the respondents to this survey explicitly\n",
    "consent to its secondary use in research (or teaching, in this case)? If\n",
    "not, can we assume implied consent? Do we need consent at all, in this\n",
    "case?*\n",
    "\n",
    "> We believe that despite the lack of an explicit consent field in the\n",
    "> questionnaire, these materials and all the supporting material in the\n",
    "> website give plenty of information and signposting to participants to\n",
    "> assure that they know the content and purpose of the survey, the\n",
    "> anonymisation process, the way data will become available and also to\n",
    "> know where to ask questions if they have concerns. It is reasonable to\n",
    "> conclude that participants that were contacted, accepted the\n",
    "> invitation, read this material, were briefed by the field worker and\n",
    "> went ahead with the interview did consent to their data being\n",
    "> collected and used. It is also clear that during this process they had\n",
    "> the option to opt out more than once. The type of intended research is\n",
    "> not made entirely clear from the forms but there are mentions to\n",
    "> improving living conditions in Europe, understanding quality of life\n",
    "> and also other examples of uses in the 2007 flyers.\n",
    "\n",
    "— *excerpt from Ethics Advisory Group practice application form\n",
    "submitted for the running of this course*\n",
    "\n",
    "The above excerpt is part of a larger document that outlines our\n",
    "application to Turing’s Ethics Advisory Group. The decision to move\n",
    "forward with the dataset was made given consideration of the paragraph\n",
    "quoted as well as points set out in the rest of the document.\n",
    "\n",
    "### Bias in, bias out\n",
    "\n",
    "We should also consider *how* and *why* the data was collected. Is it\n",
    "suitable for use in our project? Would use of this data lead us to a\n",
    "“bias in, bias out” situation with any resulting model?\n",
    "\n",
    "2018 reports of a recruiting tool at Amazon highlight a great example of\n",
    "this idea.\n",
    "\n",
    "> .. But by 2015, the company realized its new system was not rating\n",
    "> candidates for software developer jobs and other technical posts in a\n",
    "> gender-neutral way.\n",
    ">\n",
    "> That is because Amazon’s computer models were trained to vet\n",
    "> applicants by observing patterns in resumes submitted to the company\n",
    "> over a 10-year period. Most came from men, a reflection of male\n",
    "> dominance across the tech industry.\n",
    ">\n",
    "> In effect, Amazon’s system taught itself that male candidates were\n",
    "> preferable. It penalized resumes that included the word “women’s,” as\n",
    "> in “women’s chess club captain.” …\n",
    "\n",
    "— *excerpt from [Reuters article “Amazon scraps secret AI recruiting\n",
    "tool that showed bias against\n",
    "women”](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)*\n",
    "\n",
    "#### Beyond Data\n",
    "\n",
    "The idea of “bias in, bias out” may be concerned with more than simply\n",
    "the data that is used. Overarching systems, as well as programmers\n",
    "themselves, can contribute to this cycle, such as explored in Mayson\n",
    "(2018) and Cowgill et al. (2020).\n",
    "\n",
    "In Cowgill et al. (2020), results suggest that simple reminders about\n",
    "bias are effective at improving an algorithm’s accuracy. This\n",
    "intervention does not directly affect the data available to the\n",
    "programmer, implying a cause of bias beyond this factor.\n",
    "\n",
    "### Should A Variable Be Used?\n",
    "\n",
    "Questions of ethics don’t necessary stop at the dataset level. Once we\n",
    "have access to a dataset, we can examine the variables that are recorded\n",
    "in the data.\n",
    "\n",
    "Use of machine learning techniques in domains such as criminal justice,\n",
    "insurance, and financial credit raise concerns about fairness. These\n",
    "techniques often rely on historic data, containing historic biases\n",
    "against demographic groups, and can perpetuate these biases into their\n",
    "predictions (bias in, bias out!). Clearly, decisions made based on these\n",
    "predictions may be unfair. We may wish to address this by excluding\n",
    "demographic variables from the training data.\n",
    "\n",
    "Our concern here isn’t limited to machine learning, however. In\n",
    "biomedical research, for example, there is\n",
    "[debate](https://www.publichealthpost.org/research/why-are-we-still-using-race-as-a-variable-in-health-research)\n",
    "around the use of race as a variable in data analysis. Yudell et\n",
    "al. (2020) argue that, in COVID-19 analysis, reported disparities in\n",
    "cases across racial demographics can give rise to false beliefs that the\n",
    "disparities are caused by innate racial differences.\n",
    "\n",
    "``` {tip}\n",
    "Ask yourself: \"Why am I using this variable? What information does it capture and could there be other identifiers/variables better suited to my analysis and the question I want to answer?\"\n",
    "```\n",
    "\n",
    "We’ll revisit this from another angle in [2.2.7 Data\n",
    "Missingness](./2-02-07-MissingData.md).\n",
    "\n",
    "### Data Proxies\n",
    "\n",
    "We should also be aware that a variable `Y` may act as a proxy for\n",
    "another variable `X` and, in doing so, lead to the inclusion of a\n",
    "variable that we may have intended to drop.\n",
    "\n",
    "> For example, suppose a system that makes decisions about credit uses\n",
    "> zip code as a factor to make its decisions. The direct information\n",
    "> about race is not given to the system, but zip code is strongly\n",
    "> correlated with race since many neighborhoods are still segregated. By\n",
    "> using zip code, the system would be indirectly making decisions based\n",
    "> on race. In this case, zip code is a proxy for race.\n",
    "\n",
    "— *[Thwarting bias in AI\n",
    "systems](https://engineering.cmu.edu/news-events/news/2018/12/11-datta-proxies.html)*\n",
    "\n",
    "We can try to identify proxy variables by looking at data correlations\n",
    "and asking domain experts.\n",
    "\n",
    "### Algorithmic Fairness: A Trade-off\n",
    "\n",
    "Many works exist in the area of algorithmic fairness, and it is a topic\n",
    "worthy of greater discussion outside the boundary of this course. Here,\n",
    "we want to briefly highlight a tension between fairness and traditional\n",
    "measures of performance.\n",
    "\n",
    "Above, we have discussed excluding variables from the data. When we have\n",
    "a modelling task, we may find that our reported accuracy is lower\n",
    "without our excluded variables. This represents a trade-off. We should\n",
    "attempt to discern the implications of the change in accuracy and\n",
    "potentially consider these against measures of fairness.\n",
    "\n",
    "But what is “fairness”? Any answer is certainly beyond the scope of this\n",
    "course! However, a useful collection of (loose) definitions for various\n",
    "“fairness types” can be found in [Google’s What-If Tool\n",
    "documentation](https://pair-code.github.io/what-if-tool/ai-fairness.html).\n",
    "\n",
    "This trade-off is also discussed in depth, with illustrative examples,\n",
    "in Mayson (2018).\n",
    "\n",
    "## References\n",
    "\n",
    "Cowgill, B., Dell’Acqua, F., Deng, S., Hsu, D., Verma, N., & Chaintreau,\n",
    "A. (2020, July). Biased programmers? Or biased data? A field experiment\n",
    "in operationalizing AI ethics. In Proceedings of the 21st ACM Conference\n",
    "on Economics and Computation (pp. 679-681).\n",
    "\n",
    "Mayson, S. G. (2018). Bias in, bias out. YAle lJ, 128, 2218.\n",
    "\n",
    "Yudell, M., Roberts, D., DeSalle, R., Tishkoff, S., & 70 signatories.\n",
    "(2020). NIH must confront the use of race in science. Science,\n",
    "369(6509), 1313-1314."
   ],
   "id": "c807ac70-08ea-48d7-8654-1181d1845672"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
