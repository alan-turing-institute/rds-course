{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.5 Privacy and Anonymisation\n",
    "\n",
    "This section touches, again, on UK GDPR. A comprehensive guide to UK\n",
    "GDPR can be found on the [ICO\n",
    "website](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/).\n",
    "\n",
    "## Anonymisation or Pseudonymisation\n",
    "\n",
    "Pseudonymisation and anonymisation are common approaches to meet GDPR\n",
    "principles of [“data minimisation” and “storage\n",
    "limitation”](https://www.privacy-regulation.eu/en/article-5-principles-relating-to-processing-of-personal-data-GDPR.htm).\n",
    "\n",
    "Many tasks do not require the use of personal identifiers that can often\n",
    "be part of a dataset. In these cases, we should aim to remove this\n",
    "unnecessary, yet sensitive, data.\n",
    "\n",
    "### Anonymisation\n",
    "\n",
    "In a GDPR context, Recital 26 defines anonymous information as: \\>\n",
    "…information which does not relate to an identified or identifiable\n",
    "natural person or to personal data rendered anonymous in such a manner\n",
    "that the data subject is not or no longer identifiable.\n",
    "\n",
    "— *[Recital\n",
    "26](https://www.privacy-regulation.eu/en/recital-26-GDPR.htm)*\n",
    "\n",
    "Anonymisation is the process of removing direct and indirect personal\n",
    "identifiers. Crucially, post-anonymisation, subjects will not be able to\n",
    "be identified in the data, even given additional information.\n",
    "\n",
    "GDPR does not apply to anonymised information. However, note that when\n",
    "**you** anonymise personal data, **you** are still processing the data\n",
    "at that point.\n",
    "\n",
    "### Pseudonymisation\n",
    "\n",
    "> …the processing of personal data in such a manner that the personal\n",
    "> data can no longer be attributed to a specific data subject without\n",
    "> the use of additional information, provided that such additional\n",
    "> information is kept separately and is subject to technical and\n",
    "> organisational measures to ensure that the personal data are not\n",
    "> attributed to an identified or identifiable natural person.\n",
    "\n",
    "— *excerpt from [ICO Guide to Data\n",
    "Protection](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/what-is-personal-data/what-is-personal-data/#pd4)*\n",
    "\n",
    "This is subtly different to anonymisation. Here, the subject can be\n",
    "re-identified, given additional information.\n",
    "\n",
    "GDPR does still apply to pseudonymised information. However,\n",
    "pseudonymisation helps meet the “data minimisation” and “storage\n",
    "limitation” principles.\n",
    "\n",
    "## Differential Privacy\n",
    "\n",
    "Separate to concerns over data use in developing algorithms, there is\n",
    "also a concern over what may be inferred about the data by learning the\n",
    "result of some randomised algorithm. Here, the concern shifts from\n",
    "intrusion by the data scientist developing the algorithm to the\n",
    "intrusion by the many consumers of this algorithm.\n",
    "\n",
    "The below image shows a **toy version** of a membership inference\n",
    "attack. Here, an adversary attempts to infer whether certain images were\n",
    "used in the training of a cat vs dog classifier.\n",
    "\n",
    "A data scientist: 1. Trains a model on the first set of images 2.\n",
    "Publishes the model to the public internet\n",
    "\n",
    "An adversary then:\n",
    "\n",
    "1.  Makes predictions with the model through some public API\n",
    "2.  Based on the scores in the model’s predictions, loosely\n",
    "    interpretable as confidence, infers:\n",
    "    -   first image was part of the original training set\n",
    "    -   second image was not part of training training set\n",
    "\n",
    "% on imgur due to size ![toy membership inference\n",
    "figure](https://i.imgur.com/ujb8iPk.jpg)\n",
    "\n",
    "Broadly, differential privacy provides a mechanism for learning nothing\n",
    "about an individual while learning useful information about the general\n",
    "population.\n",
    "\n",
    "> “Differential privacy” describes a promise, made by a data holder, or\n",
    "> curator, to a data subject: “You will not be affected, adversely or\n",
    "> otherwise, by allowing your data to be used in any study or analysis,\n",
    "> no matter what other studies, data sets, or information sources, are\n",
    "> available.”\n",
    "\n",
    "— *[The Algorithmic Foundations of Differential\n",
    "Privacy](https://www.tau.ac.il/~saharon/BigData2018/privacybook.pdf)* -\n",
    "Dwork & Roth (2014)\n",
    "\n",
    "Use of differential privacy in data science and machine learning is an\n",
    "ongoing area of research. We recommend the 2018 blog post, [Privacy and\n",
    "machine learning: two unexpected\n",
    "allies?](http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html),\n",
    "as further reading.\n",
    "\n",
    "## References\n",
    "\n",
    "Dwork, C., & Roth, A. (2014). The algorithmic foundations of\n",
    "differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4),\n",
    "211-407."
   ],
   "id": "2b3604b4-2d1a-4a22-a16e-cbd32494c19c"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
