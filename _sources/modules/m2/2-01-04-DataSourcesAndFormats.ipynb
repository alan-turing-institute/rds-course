{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b8d3c3",
   "metadata": {},
   "source": [
    "# 2.1.4 Data Sources and Formats\n",
    "\n",
    "Once you've found a dataset for your research question there are many different formats it could be in - tabular data, databases, documents, images or many more. In this section we give an overview of common data types and how they can be loaded into Python.\n",
    "\n",
    "Several of the file/data formats here are also covered in our [Research Software Engineering course](https://alan-turing-institute.github.io/rsd-engineeringcourse/). Here we show how to load data with Pandas, and also introduce some new data types.\n",
    "\n",
    "## Tabular Data Files\n",
    "\n",
    "### Comma-separated values files (CSVs)\n",
    "\n",
    "Tabular data files, and particularly comma-separated values (CSV) files, are likely to be the data format you encounter most often. These specify a single table of rows and columns in the following format:\n",
    "\n",
    "```\n",
    "\"Subject\",\"Teacher\",\"Day of Week\",\"Time Start\",\"Time End\"\n",
    "\"Maths\",\"Mr F\",\"Monday\",1000,1200\n",
    "\"English\",\"Ms P\",\"Tuesday\",1100,1300\n",
    "\"Physics\",\"Mrs A\",\"Thursday\",1400,1600\n",
    "\"French\",\"Mr F\",\"Friday\",1000,1300\n",
    "```\n",
    "\n",
    "The first line (usually) specifies the table column names, with each name separated by a comma. The subsequent lines specify the values of those columns for each row in our data, with each column  value separated by a comma.\n",
    "\n",
    "As an example, we will use a dataset downloaded from The World Bank, giving the percentage of people living in urban environments by country worldwide since 1960. You can find the original source data [here](https://data.worldbank.org/indicator/SP.URB.TOTL.IN.ZS?view=chart).\n",
    "\n",
    "To load a CSV file to a Pandas data frame you can use the Pandas [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b919c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/urban_population.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e3754",
   "metadata": {},
   "source": [
    "Let's look at the first 5 rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7108e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1980</th>\n",
       "      <th>2000</th>\n",
       "      <th>2020</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>50.776000</td>\n",
       "      <td>50.472000</td>\n",
       "      <td>46.717000</td>\n",
       "      <td>43.697000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>14.704688</td>\n",
       "      <td>20.845000</td>\n",
       "      <td>28.669286</td>\n",
       "      <td>36.783306</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>8.401000</td>\n",
       "      <td>15.995000</td>\n",
       "      <td>22.078000</td>\n",
       "      <td>26.026000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Western and Central</td>\n",
       "      <td>AFW</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>14.670329</td>\n",
       "      <td>24.518577</td>\n",
       "      <td>35.352981</td>\n",
       "      <td>47.848625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>10.435000</td>\n",
       "      <td>24.298000</td>\n",
       "      <td>50.087000</td>\n",
       "      <td>66.825000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Name Country Code  \\\n",
       "0                        Aruba          ABW   \n",
       "1  Africa Eastern and Southern          AFE   \n",
       "2                  Afghanistan          AFG   \n",
       "3   Africa Western and Central          AFW   \n",
       "4                       Angola          AGO   \n",
       "\n",
       "                             Indicator Name     Indicator Code       1960  \\\n",
       "0  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  50.776000   \n",
       "1  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  14.704688   \n",
       "2  Urban population (% of total population)  SP.URB.TOTL.IN.ZS   8.401000   \n",
       "3  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  14.670329   \n",
       "4  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  10.435000   \n",
       "\n",
       "        1980       2000       2020  Unnamed: 8  \n",
       "0  50.472000  46.717000  43.697000         NaN  \n",
       "1  20.845000  28.669286  36.783306         NaN  \n",
       "2  15.995000  22.078000  26.026000         NaN  \n",
       "3  24.518577  35.352981  47.848625         NaN  \n",
       "4  24.298000  50.087000  66.825000         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3550d55",
   "metadata": {},
   "source": [
    "Each row in the data corresponds to a country, with columns for the country's name, the urban population in 1960, 1980, 2000 and 2020, and some other metadata.\n",
    "\n",
    "The `.info()` method of a DataFrame gives us a useful summary of the columns it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75e177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266 entries, 0 to 265\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Country Name    266 non-null    object \n",
      " 1   Country Code    266 non-null    object \n",
      " 2   Indicator Name  266 non-null    object \n",
      " 3   Indicator Code  266 non-null    object \n",
      " 4   1960            263 non-null    float64\n",
      " 5   1980            263 non-null    float64\n",
      " 6   2000            263 non-null    float64\n",
      " 7   2020            262 non-null    float64\n",
      " 8   Unnamed: 8      0 non-null      float64\n",
      "dtypes: float64(5), object(4)\n",
      "memory usage: 18.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28404878",
   "metadata": {},
   "source": [
    "Each column in a Pandas DataFrame has a single type. The urban population percentages in columns 1960, 1980, 2000 and 2020 all contain floating point numbers (`float64`), for example. When columns contain a mixture of data types or strings, Pandas will give the whole column the generic `object` type. Sometimes quirks in the data may cause Pandas to infer a different type to what you expect - we will revisit this in the data wrangling section.\n",
    "\n",
    "We can also see that some columns have missing values (the data has 266 rows, but some columns  have fewer than 266 \"`non-null`\" values), and have a strange additional column `Unnamed: 8` that has _only_ missing (null) values. This is another topic we'll revisit later.\n",
    "\n",
    "### Customising pandas.read_csv\n",
    "\n",
    "The original file from the World Bank contains a few lines of metadata at the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957f2897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Data Source\",\"World Development Indicators\",\r\n",
      "\r\n",
      "\"Last Updated Date\",\"2021-06-30\",\r\n",
      "Country Name,Country Code,Indicator Name,Indicator Code,1960,1980,2000,2020,\r\n",
      "Aruba,ABW,Urban population (% of total population),SP.URB.TOTL.IN.ZS,50.776,50.472,46.717,43.697,\r\n",
      "Africa Eastern and Southern,AFE,Urban population (% of total population),SP.URB.TOTL.IN.ZS,14.7046880270389,20.8449997980123,28.6692864525936,36.7833061490919,\r\n",
      "Afghanistan,AFG,Urban population (% of total population),SP.URB.TOTL.IN.ZS,8.401,15.995,22.078,26.026,\r\n",
      "Africa Western and Central,AFW,Urban population (% of total population),SP.URB.TOTL.IN.ZS,14.6703287907718,24.5185774336299,35.3529813285238,47.8486254352506,\r\n",
      "Angola,AGO,Urban population (% of total population),SP.URB.TOTL.IN.ZS,10.435,24.298,50.087,66.825,\r\n",
      "Albania,ALB,Urban population (% of total population),SP.URB.TOTL.IN.ZS,30.705,33.762,41.741,62.112,\r\n"
     ]
    }
   ],
   "source": [
    "!head data/urban_population_header.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79107b68",
   "metadata": {},
   "source": [
    "The column names start on line 4 of the file, and the previous lines give metadata on where the file came from and when it was updated.\n",
    "\n",
    "Using `read_csv` on this file (with default arguments) gives an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b15f154",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 4, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2000/270367433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/urban_population_header.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/hostedtoolcache/Python/3.9.8/x64/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 4, saw 9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/urban_population_header.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963d8b3",
   "metadata": {},
   "source": [
    "This is because pandas is trying to use the first line in the file to define the columns present in our data. To avoid this, we can use the `skiprows` argument to tell pandas our table starts on line 4 (skipping the first 3 lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171eaac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1980</th>\n",
       "      <th>2000</th>\n",
       "      <th>2020</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>ABW</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>50.776000</td>\n",
       "      <td>50.472000</td>\n",
       "      <td>46.717000</td>\n",
       "      <td>43.697000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>14.704688</td>\n",
       "      <td>20.845000</td>\n",
       "      <td>28.669286</td>\n",
       "      <td>36.783306</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>8.401000</td>\n",
       "      <td>15.995000</td>\n",
       "      <td>22.078000</td>\n",
       "      <td>26.026000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Western and Central</td>\n",
       "      <td>AFW</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>14.670329</td>\n",
       "      <td>24.518577</td>\n",
       "      <td>35.352981</td>\n",
       "      <td>47.848625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Urban population (% of total population)</td>\n",
       "      <td>SP.URB.TOTL.IN.ZS</td>\n",
       "      <td>10.435000</td>\n",
       "      <td>24.298000</td>\n",
       "      <td>50.087000</td>\n",
       "      <td>66.825000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Name Country Code  \\\n",
       "0                        Aruba          ABW   \n",
       "1  Africa Eastern and Southern          AFE   \n",
       "2                  Afghanistan          AFG   \n",
       "3   Africa Western and Central          AFW   \n",
       "4                       Angola          AGO   \n",
       "\n",
       "                             Indicator Name     Indicator Code       1960  \\\n",
       "0  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  50.776000   \n",
       "1  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  14.704688   \n",
       "2  Urban population (% of total population)  SP.URB.TOTL.IN.ZS   8.401000   \n",
       "3  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  14.670329   \n",
       "4  Urban population (% of total population)  SP.URB.TOTL.IN.ZS  10.435000   \n",
       "\n",
       "        1980       2000       2020  Unnamed: 8  \n",
       "0  50.472000  46.717000  43.697000         NaN  \n",
       "1  20.845000  28.669286  36.783306         NaN  \n",
       "2  15.995000  22.078000  26.026000         NaN  \n",
       "3  24.518577  35.352981  47.848625         NaN  \n",
       "4  24.298000  50.087000  66.825000         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/urban_population_header.csv\", skiprows=3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d187576",
   "metadata": {},
   "source": [
    "There's not a single CSV data \"standard\" that everyone follows, so it's common to need to tweak things to load properly. Other examples include:\n",
    "\n",
    "- Different \"delimeters\", e.g., Tab-separated values files (TSVs) with tabs separating column values instead of commas.\n",
    "- Footers: Lines with metadata/other information not part of the table at the end of the file.\n",
    "- Comments: Annotations anywhere in the file that shouldn't be included in the table.\n",
    "\n",
    "`pd.read_csv` can be customised to deal with all of these and more, for example:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\n",
    "   \"myfile.csv\",\n",
    "   delimeter=\"\\t\",  # columns separated by tabs\n",
    "   skipfooter=3,    # exclude last 3 lines\n",
    "   comment=\"#\",     # exclude lines starting with \"#\"\n",
    ")\n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "Later on we'll be using the \"Iris\" dataset, which we have saved at the path `data/Iris.csv`. Load this dataset into a pandas data frame. What type of data do you think it contains? What are the types of the columns and do they match what you'd expect?\n",
    "```\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: dropdown tip\n",
    "`df = pd.read_csv(\"data/Iris.csv\")` to load the data into pandas. We'll discuss the answer to the other questions later, but you may have noticed that some of the columns have the generic `object` type even though it looks like they contain numeric data.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Excel Spreadsheets\n",
    "\n",
    "As CSV files are a plaintext (human readable) format and don't need proprietry software to use and create they are always preferable to Excel spreadsheets for raw data, if available. However, it is still common to find data in Excel `.xls` or `.xlsx` files.\n",
    "\n",
    "If needed, Pandas also has a function [`pandas.read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) to load data from Excel spreadsheets. The format is very similar to `read_csv`:\n",
    "\n",
    "```python\n",
    "df = pd.read_excel(\n",
    "   \"my_spreadsheet.xlsx\",\n",
    "   sheet_name=\"Sheet1\"\n",
    ")\n",
    "```\n",
    "\n",
    "Note you must specify which worksheet you want to load as a data frame. If you want to load tables from multiple worksheets you would need to load them into multiple data frames.\n",
    "\n",
    "## Databases\n",
    "\n",
    "We can think of databases as containers for data. Typically, databases allow for easy storage, retrieval, update, and analysis of data.\n",
    "\n",
    "In the case of tabular data, a database may have many tables. A database can be flat, or it can be relational.\n",
    "\n",
    "Example:\n",
    "\n",
    "**Flat Database with single table:**\n",
    "\n",
    "| Subject   | Teacher | Teacher Email | Teacher Phone Number | Day of Week | Time Start | Time End |\n",
    "|-----------|---------|---------------|----------------------|-------------|------------|----------|\n",
    "| Maths     | Mr F    | f@school.com   | 07123123123          | Monday      | 1000       | 1200     |\n",
    "| English   | Ms P    | p@school.com  | 07456456456          | Tuesday     | 1100       | 1300     |\n",
    "| Physics   | Mrs A   | a@school.com  | 07789789789          | Thursday    | 1400       | 1600     |\n",
    "| French | Mr F    | f@school.co   | 07123123123          | Friday      | 1000       | 1300     |\n",
    "\n",
    "**Relational database with two tables:**\n",
    "\n",
    "| Subject   | Teacher | Day of Week | Time Start | Tie End |\n",
    "|-----------|---------|-------------|------------|---------|\n",
    "| Maths     | Mr F    | Monday      | 1000       | 1200    |\n",
    "| English   | Ms P    | Tuesday     | 1100       | 1300    |\n",
    "| Physics   | Mrs A   | Thursday    | 1400       | 1600    |\n",
    "| French | Mr F    | Friday      | 1000       | 1300    |\n",
    "\n",
    "| Teacher | Teacher Email | Teacher Phone Number |\n",
    "|---------|---------------|----------------------|\n",
    "| Mr F    | f@school.com   | 07123123123          |\n",
    "| Ms P    | p@school.com  | 07456456456          |\n",
    "| Mrs A   | a@school.com  | 07789789789          |\n",
    "\n",
    "Flat databases can be simple but also inefficient. Relational databases, with data stored in a series of interconnected tables, can be more complicated but more computationally efficient.\n",
    "\n",
    "### SQL Databases & RDBMS\n",
    "\n",
    "Structured Query Lanage (SQL) is a standard language for storing, manipulating and retrieving data in databases.\n",
    "\n",
    "An RDBMS (Relational Database Management System) is the basis for SQL and relies on the [relational data model](https://en.wikipedia.org/wiki/Relational_model). Many modern and popular databases implement the ideas of an RDBMS. Some common implementations are:\n",
    "\n",
    "- PostgreSQL\n",
    "- SQLite\n",
    "- MySQL\n",
    "- Oracle\n",
    "- MS Access\n",
    "\n",
    "We use SQL to interact with these databases. You can find some basic, interactive, tutorials at [W3Schools](https://www.w3schools.com/sql/).\n",
    "\n",
    "\n",
    "\n",
    "### NoSQL Databases\n",
    "\n",
    "> NoSQL databases (aka \"not only SQL\") are non tabular, and store data differently than relational tables. NoSQL databases come in a variety of types based on their data model. The main types are document, key-value, wide-column, and graph. They provide flexible schemas and scale easily with large amounts of data and high user loads.\n",
    "*https://www.mongodb.com/nosql-explained*\n",
    "\n",
    "Some common NoSQL Databases include:\n",
    "\n",
    "- Elasticsearch (document)\n",
    "- MongoDB (document)\n",
    "- DynamoDB (key-value)\n",
    "- Gaffer (graph)\n",
    "\n",
    "**Document example**\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Jane\",\n",
    "    \"dob\": \"2012-04-23T18:25:43.511Z\",\n",
    "    \"interests\": [\"running\", \"swimming\"],\n",
    "    \"counter\": 104\n",
    "}\n",
    "```\n",
    "\n",
    "**Key-Value example:**\n",
    "\n",
    "![key-value example](https://upload.wikimedia.org/wikipedia/commons/5/5b/KeyValue.PNG)\n",
    "\n",
    "**Graph example:**\n",
    "![graph example](../../figures/m2/graph_data.png)\n",
    "\n",
    "\n",
    "Unfortunately, different types of databases will often have their own, unique, query languages, beyond SQL - more to learn!\n",
    "\n",
    "\n",
    "### Where Is the Data?\n",
    "\n",
    "Databases can exist on your local machine or can be hosted elsewhere. Typically, we'll want to host a database somewhere that everyone who needs access can reach.\n",
    "\n",
    "Many cloud providers offer database solutions with easy-to-use interfaces, allowing us (for a cost!) to worry less about exactly how the DBMS is working and focus, instead, on the data itself.\n",
    "\n",
    "\n",
    "### Which Database to Use\n",
    "\n",
    "Which type of database you choose should be driven by your data and how you plan to use it.\n",
    "You may find that your data can be expressed in more than one of the forms above or as a table.\n",
    "You then need to weigh up different factors to decide which form and which database, if any, to use.\n",
    "\n",
    " Some factors you may consider (unordered!):\n",
    "\n",
    " - Ease of use\n",
    " - Cost\n",
    " - Footprint on disk\n",
    " - Footprint in memory\n",
    " - Retrieval/update speed\n",
    "\n",
    "Many blogs and articles have been written on the choice of database, for example [this xplenty article](https://www.xplenty.com/blog/which-database/).\n",
    "\n",
    "## Application Programming Interfaces (APIs)\n",
    "\n",
    "Application programming interfaces (APIs) are services that allow you to programmatically request information, including datasets, from a remote server. Large, digitally mature organisations commonly have APIs available to access their data, including [GitHub](https://docs.github.com/en/rest/guides/getting-started-with-the-rest-api), [Twitter](https://developer.twitter.com/en/docs/twitter-api) and the [Office for National Statistics](https://developer.ons.gov.uk/), for example. Advantages of APIs include:\n",
    "\n",
    "- It's possible to make queries to request a tailored subset of a dataset, so you only get what you're interested in.\n",
    "- It's  straightforward to automate making many similar queries (i.e., to request many different datasets) with different parameters.\n",
    "- If a dataset is updating regularly (maybe you want to know the current weather at a location, for example) they provide a robust way to get the latest status of what you're interested in.\n",
    "- They usually should provide a reproducible way for others to download the same data you're using.\n",
    "\n",
    "Most APIs follow the REST style, which we briefly  introduce here, but you may also come across GraphQL APIs. [This blog](https://www.smashingmagazine.com/2018/01/understanding-using-rest-api/) has an overview of both types.\n",
    "\n",
    "### Datamuse API\n",
    "\n",
    "As an example, we'll try the Datamuse API, which can be used to find relationships between words and terms (similar words, words that rhyme and so on). You can find documentation for it here: https://www.datamuse.com/api/\n",
    "\n",
    "An API query has four main components: a base URL, an endpoint, parameters, and headers:\n",
    "\n",
    "**Base URL:** The address of the server hosting the data we want to access - all our API queries will start with this. For the Datamuse API it's: `https://api.datamuse.com`\n",
    "\n",
    "**Endpoint:** Each API might have multiple types of data we can query, known as different \"endpoints\". The path to the endpoint is added to the base URL. The Datamuse API has two endpoints:`/words` for searching for relationships (of different types) between words, and `/sug`, which can be used for auto-completion/spelling correction.\n",
    "\n",
    "\n",
    "**Parameters:** Define what data subset we're interested in and are provided in the format `name=value`. These are added to the query after a `?` following the endpoint. To specify multiple parameters, separate each parameter name and value pair with an `&`. The available parameters should be listed in the API's documentation ([here](https://www.datamuse.com/api/) for Datamuse).\n",
    "\n",
    "The parameter string `?rel_jjb=dog&max=5` defines two parameters and values:\n",
    "- `rel_jjb=dog`: Find adjectives commonly used to describe the noun \"dog\".\n",
    "- `max=5`: Return 5 results.\n",
    "\n",
    "**Headers:** Provide additional context/metadata about your request, the most common being authentication information. Authentication may be used to only give you access to resources you have permission to see, limit the number of requests you can make within a certain amount of time, or for billing purposes (if the API isn't free).\n",
    "\n",
    "The Datamuse API doesn't require authentication, so we don't need to add any headers here. If the API you're using requires it they should provide documentation that describes what you need to do - [here is an example from GitHub](https://docs.github.com/en/rest/guides/getting-started-with-the-rest-api#authentication), and [here is how to add headers in the Python requests library](https://docs.python-requests.org/en/master/user/quickstart/#custom-headers).\n",
    "\n",
    "**Full query:** Adding together all the components  we have the full API query:\n",
    "- https://api.datamuse.com/words?rel_jjb=dog&max=5\n",
    "\n",
    "You can open this in a web browser and should see something like this:\n",
    "```json\n",
    "[\n",
    "    {\"word\": \"little\", \"score\": 1001},\n",
    "    {\"word\": \"old\", \"score\": 1000},\n",
    "    {\"word\": \"hot\", \"score\": 999},\n",
    "    {\"word\": \"big\", \"score\": 998},\n",
    "    {\"word\": \"black\", \"score\": 997}\n",
    "]\n",
    "```\n",
    "\n",
    "We get the top 5 words describing \"dog\" and a score that indicates how strong the association is.\n",
    "\n",
    "The most common format for data returned by an API is JSON (JavaScript Object Notation) from the JavaScript language, which is commonly used in web development. JSONs have a similar structure to Python dictionaries or lists of Python dictionaries. Python has a built-in library `json` for converting data to and from the JSON format (see [here](https://docs.python.org/3/library/json.html)), and Pandas also has a function for creating a DataFrame from a JSON file ([pandas.read_json](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html)).\n",
    "\n",
    "We can make the same API query in Python using the\n",
    "[requests](https://docs.python-requests.org/en/master/user/quickstart/) library, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993d287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.datamuse.com/words\"\n",
    "params = {\"rel_jjb\": \"dog\", \"max\": 5}\n",
    "r = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ca129",
   "metadata": {},
   "source": [
    "Note that we can define the parameters in a dictionary, which is much easier to read than the raw format in the query string seen earlier. To check whether the request worked you can check the status code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493c5f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cd617",
   "metadata": {},
   "source": [
    "Codes in the 200s usually indicate a successful query, for the meanings of other codes see [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), or `print(r.content)` may give you more information about what happened.\n",
    "\n",
    "We can convert the result into a list of dictionaries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'little', 'score': 1001}, {'word': 'old', 'score': 1000}, {'word': 'hot', 'score': 999}, {'word': 'big', 'score': 998}, {'word': 'black', 'score': 997}]\n"
     ]
    }
   ],
   "source": [
    "result_list = r.json()\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e9174",
   "metadata": {},
   "source": [
    "And we can interact with that list in the usual Python way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42492ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little\n"
     ]
    }
   ],
   "source": [
    "print(result_list[0][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf81c73",
   "metadata": {},
   "source": [
    "You can also load an API query directly into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2cc726c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hot</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  score\n",
       "0  little   1001\n",
       "1     old   1000\n",
       "2     hot    999\n",
       "3     big    998\n",
       "4   black    997"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"https://api.datamuse.com/words?rel_jjb=dog&max=5\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038ac5d",
   "metadata": {},
   "source": [
    "But this may not work well if your query returns a more complex data structure - in that case it's best to start with the requests library.\n",
    "\n",
    "Bear in mind it's likely the service you're using will limit both the rate of queries you can make, and the amount of data returned per query. If you want more data than can be returned by one query, an API will usually provide a way to get the data with multiple queries - this is known as \"pagination\" (see [this blog post](https://nordicapis.com/everything-you-need-to-know-about-api-pagination/) for example). If you're making many queries you may need to limit their rate in your script to avoid breaching the usage rules of the API - a quick way to do that would be to use the [time.sleep](https://realpython.com/python-sleep/#adding-a-python-sleep-call-with-timesleep) function in Python.\n",
    "\n",
    "This \"public-apis\" repository on GitHub maintains a list of open APIs that should be good for learning purposes: https://github.com/public-apis/public-apis. The UK government also maintains a list of APIs providing data on many different areas of government & life in the UK: https://www.api.gov.uk/\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "Use the Datamuse API to find 3 words that rhyme with \"cat\" and load them into a Pandas data frame. You'll need to use the Datamuse documnetation to find which parameter will search for rhyming words.\n",
    "```\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: dropdown tip\n",
    "The relevant parameter is `rel_rhy`, and the query and pandas function needed is `pd.read_json(\"https://api.datamuse.com/words?rel_rhy=cat&max=3\")`\n",
    "```\n",
    "\n",
    "## Image Data\n",
    "\n",
    "Images are commonly represented as a n-dimensional tensor of pixel values.\n",
    "\n",
    "### Black and White\n",
    "\n",
    "A simple form of this can be shown with a black and white image. The 13x13 pixel smiley can be represented by a 2D tensor (width,height) of 0s and 1s. Black and white images can be thought of as a binary off/on for each pixel.\n",
    "\n",
    "\n",
    "![black_and_white_smiley](../../figures/m2/black_and_white_smiley.png)\n",
    "\n",
    "### Grayscale\n",
    "\n",
    "If we want to move to grayscale - allowing shades of gray between black and white - we can do this with intermediate values between black and white. Here we change black and white to 0 and 255 (8-bit representation), shades of gray are everything in between.\n",
    "\n",
    "\n",
    "![grayscale_smiley](../../figures/m2/grayscale_smiley.png)\n",
    "\n",
    "\n",
    "### RGB\n",
    "\n",
    "We can introduce colour using an RGB (Red Green Blue) representation. Here, we store the red, green, and blue values separately - these individual representations are known as channels. We now use a 3D tensor to represent this image (width,height,channels).\n",
    "\n",
    "% hosted on imgur for size\n",
    "![rgb_smiley](https://i.imgur.com/Z0Z52Ph.gif)\n",
    "\n",
    "### Libraries\n",
    "\n",
    "Some popular libraries for processing and analysing image data in Python include:\n",
    "\n",
    "- [opencv-python](https://pypi.org/project/opencv-python/): [OpenCV](https://opencv.org/) (Open-source Computer Vision) packages for Python. Contains hundreds of computer vision algorithms.\n",
    "- [Pillow](https://pillow.readthedocs.io/en/stable/): PIL (Python Imaging Library) fork. \"This library provides extensive file format support, an efficient internal representation, and fairly powerful image processing capabilities\".\n",
    "- [torchvision](https://pytorch.org/vision/stable/index.html): Part of the wider [PyTorch](http://pytorch.org/) project. \"The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision\".\n",
    "- [tf.keras.preprocessing](https://www.tensorflow.org/tutorials/load_data/images#load_using_tfkeraspreprocessing): Part of the wider tensorflow/keras ecosystem, the preprocessing module provides preprocessing utilities for image data.\n",
    "\n",
    "## Text Data\n",
    "\n",
    "Text data is common in data science applications, anything from analysing Tweets and social media to documents and novels. We'll cover some general techniques for handling text data in this module, but we won't delve into the details of the wider field of \"Natural Language Processing\" (NLP). Some popular libraries and resources for processing and analysing text data in Python include:\n",
    "\n",
    "- [NLTK](https://www.nltk.org/): Well-established natural language processing toolkit for Python, offering a wide range of text processing techniques in many different languages.\n",
    "- [Spacy](https://spacy.io/): A more modern alternative to NLTK, offering higher performance, state of the art algorithms, and better integration with modelling frameworks in some cases.\n",
    "- [Scikit-Learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html): Has common NLP preprocessing functions which can be integrated with modelling pipelines using many different machine learning algorithms.\n",
    "- [Hugging Face](https://huggingface.co/): For implementations of state-of-the-art deep learning models from research.\n",
    "- [Textract](https://textract.readthedocs.io/en/stable/): Text data may appear in many file formats other than plaintext files (`.txt` or similar), such as in PDFs, word documents, or even within images. Textract provides a single interface for extracting text data from many different formats.\n",
    "\n",
    "## Other\n",
    "\n",
    "- Audio: commonly stored as `.wav` or `.mp3` and displayed as a [waveform](https://en.wikipedia.org/wiki/Waveform) - digitised audio can be used for things like training speech recognition models. Lots of crossover with signal processing! [Librosa](https://librosa.org/doc/latest/index.html) is a popular python package for working with audio.\n",
    "- Video: can be thought of as many images + audio!\n",
    "- Geospatial: data relating to any location on the Earth's surface. The [geopandas](https://geopandas.org/) library combines the capabilities of [shapely](https://shapely.readthedocs.io/) and Pandas to make working with this datatype easier.\n",
    "- Time Series: Data that can be expressed as observations over time, e.g., stock price data. Honourable mention to [sktime](https://github.com/alan-turing-institute/sktime) for analysis.\n",
    "- XML (Extensible Markup Language): We recommend using a library such as [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to help parse this heavily structured format.\n",
    "- Web scraping: extracting structured data from web sites. [Scrapy](https://scrapy.org/) is a popular library here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d66b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "source_map": [
   14,
   42,
   46,
   50,
   52,
   58,
   60,
   70,
   72,
   78,
   82,
   86,
   89,
   295,
   301,
   305,
   307,
   313,
   316,
   320,
   322,
   326,
   331,
   403
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}